apiVersion: v1
kind: Template
metadata:
  name: selectorsyncset-template
objects:
- apiVersion: hive.openshift.io/v1
  kind: SelectorSyncSet
  metadata:
    generation: 1
    labels:
      managed.openshift.io/gitHash: e746757
      managed.openshift.io/osd: 'true'
    name: osd-managed-prometheus-exporter-stuck-ebs-vols
  spec:
    clusterDeploymentSelector:
      matchLabels:
        api.openshift.com/managed: 'true'
        hive.openshift.io/cluster-platform: aws
    resourceApplyMode: Sync
    resources:
    - apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: sre-stuck-ebs-vols
        namespace: openshift-monitoring
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: RoleBinding
      metadata:
        name: sre-stuck-ebs-vols
        namespace: openshift-monitoring
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: edit
      subjects:
      - kind: ServiceAccount
        name: sre-stuck-ebs-vols
        namespace: openshift-monitoring
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        name: sre-allow-read-machine-info
      rules:
      - apiGroups:
        - machine.openshift.io
        resources:
        - machines
        verbs:
        - get
        - list
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: RoleBinding
      metadata:
        name: sre-stuck-ebs-vols-read-machine-info
        namespace: openshift-machine-api
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: sre-allow-read-machine-info
      subjects:
      - kind: ServiceAccount
        name: sre-stuck-ebs-vols
        namespace: openshift-monitoring
    - apiVersion: cloudcredential.openshift.io/v1
      kind: CredentialsRequest
      metadata:
        name: sre-stuck-ebs-vols-aws-credentials
        namespace: openshift-monitoring
      spec:
        providerSpec:
          apiVersion: cloudcredential.openshift.io/v1
          kind: AWSProviderSpec
          statementEntries:
          - action:
            - ec2:DescribeInstances
            effect: Allow
            resource: '*'
        secretRef:
          name: sre-stuck-ebs-vols-aws-credentials
          namespace: openshift-monitoring
    - apiVersion: v1
      data:
        main.py: "#!/usr/bin/python\n\nimport openshift as oc\nimport time\nimport\
          \ os\nimport sys\nimport re\nimport logging\n\nfrom prometheus_client import\
          \ start_http_server, Enum, Counter\n\nimport boto3\nimport botocore\n\n\
          MONITOR_NAME = \"sre-stuck-ebs-volume\"\nPROJECT = \"openshift-monitoring\"\
          \nVALID_STATES = [ \"attaching\", \"attached\", \"detaching\", \"detached\"\
          \ ]\n\n#clusterid is included to better support future Prometheus federation.\n\
          VOLUME_STATE = Enum('ebs_volume_state','EBS Volume state',[\"vol_name\"\
          ,\"clusterid\"],\n                states = VALID_STATES)\n\n# A list (implemented\
          \ as a Set) of all non-deleted volumes.\n# After we get a list of all volume\
          \ IDs from our running instances we will run\n# a query against the API\
          \ for the volumes we know about and prune as needed.\nACTIVE_VOLUMES = set([])\n\
          \nBOTO_ERRS = Counter('boto_exceptions', 'The total number of boto exceptions')\n\
          \ndef normalize_prometheus_label(str):\n    \"\"\"\n    Prometheus labels\
          \ must match /[a-zA-Z_][a-zA-Z0-9_]*/ and so we should coerce our data to\
          \ it.\n    Source: https://prometheus.io/docs/concepts/data_model/\n   \
          \ Every invalid character will be made to be an underscore `_`.\n    \"\"\
          \"\n    return re.sub(r'[^[a-zA-Z_][a-zA-Z0-9_]*]',\"_\",str,0)\n\ndef check_ebs_volumes_for_cluster(aws,clusterid):\n\
          \    \"\"\"\n    Get all volumes from AWS, but only those in use by our\
          \ cluster, then dump the metrics.\n    Note: It is important to filter by\
          \ cluster id to prevent two clusters sharing\n    an account each paging\
          \ for the same volume.\n    \"\"\"\n\n    # get the instances that are in\
          \ use by our cluster.\n    instances = aws.describe_instances(Filters=[\n\
          \        {\n            'Name':   'tag:kubernetes.io/cluster/' + clusterid,\n\
          \            'Values': ['owned']\n        }\n    ])\n    normalized_clusterid\
          \ = normalize_prometheus_label(clusterid)\n\n    # the volumes we've seen\
          \ on this iteration. later, if we haven't seen a volume,\n    # we will\
          \ purge it from VOLUME_STATES\n    seen_volumes = set([])\n\n    # iterate\
          \ through all the volumes\n    for reservation in instances[\"Reservations\"\
          ]:\n        for instance in reservation[\"Instances\"]:\n            for\
          \ block_device_map in instance[\"BlockDeviceMappings\"]:\n             \
          \   if block_device_map[\"Ebs\"][\"Status\"] not in VALID_STATES:\n    \
          \                logging.warning(\"clusterid='%s', vol_name='%s' in unknown\
          \ state. Got state='%s'\",clusterid,block_device_map[\"Ebs\"][\"VolumeId\"\
          ],block_device_map[\"Ebs\"][\"Status\"])\n                else:\n      \
          \              normalized_vol_id = normalize_prometheus_label(block_device_map[\"\
          Ebs\"][\"VolumeId\"])\n                    # Add the volume to the set\n\
          \                    ACTIVE_VOLUMES.add(block_device_map[\"Ebs\"][\"VolumeId\"\
          ])\n                    seen_volumes.add(block_device_map[\"Ebs\"][\"VolumeId\"\
          ])\n                    VOLUME_STATE.labels(normalized_vol_id,normalized_clusterid).state(block_device_map[\"\
          Ebs\"][\"Status\"])\n\n    for inactive_volume in ACTIVE_VOLUMES - seen_volumes:\n\
          \        logging.info(\"Removing vol_name='%s' for clusterid='%s' from Prometheus\
          \ \",inactive_volume,clusterid)\n        VOLUME_STATE.remove(normalize_prometheus_label(inactive_volume),normalized_clusterid)\n\
          \        ACTIVE_VOLUMES.remove(inactive_volume)\n\nif __name__ == '__main__':\n\
          \    logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s:%(name)s:%(message)s')\n\
          \    clusterid = \"\"\n    if \"AWS_CONFIG_FILE\" not in os.environ:\n \
          \       logging.error(\"Expected to have AWS_CONFIG_FILE set in the environment.\
          \ Exiting...\")\n        exit(1)\n    if \"AWS_SHARED_CREDENTIALS_FILE\"\
          \ not in os.environ:\n        logging.error(\"Expected to have AWS_SHARED_CREDENTIALS_FILE\
          \ set in the environment. Exiting...\")\n        exit(1)\n    if \"CLUSTERID\"\
          \ not in os.environ:\n        logging.error(\"Expected to have CLUSTERID\
          \ in environment. Exiting\")\n        exit(1)\n    clusterid = os.environ.get(\"\
          CLUSTERID\")\n\n    aws = boto3.client('ec2')\n\n    logging.info('Starting\
          \ up metrics endpoint')\n    # Start up the server to expose the metrics.\n\
          \    start_http_server(8080)\n    while True:\n        try:\n          \
          \  check_ebs_volumes_for_cluster(aws,clusterid)\n        except botocore.exceptions.ClientError\
          \ as err:\n            BOTO_ERRS.inc()\n            logging.error(\"Caught\
          \ boto error\")\n            logging.error('Error Message: {}'.format(err.response['Error']['Message']))\n\
          \            logging.error('Request ID: {}'.format(err.response['ResponseMetadata']['RequestId']))\n\
          \            logging.error('Http code: {}'.format(err.response['ResponseMetadata']['HTTPStatusCode']))\n\
          \        finally:\n            time.sleep(60)\n"
        start.sh: "#!/bin/sh\n\nset -o allexport\n\nif [[ -d /config && -d /config/env\
          \ ]]; then\n  source /config/env/*\nfi\n\nexec /usr/bin/python /monitor/main.py\
          \ \"$@\""
      kind: ConfigMap
      metadata:
        creationTimestamp: null
        name: sre-stuck-ebs-vols-code
        namespace: openshift-monitoring
    - apiVersion: apps.openshift.io/v1
      kind: DeploymentConfig
      metadata:
        labels:
          name: sre-stuck-ebs-vols
        name: sre-stuck-ebs-vols
        namespace: openshift-monitoring
      spec:
        paused: false
        replicas: 1
        selector:
          name: sre-stuck-ebs-vols
        strategy:
          type: Recreate
        template:
          metadata:
            labels:
              name: sre-stuck-ebs-vols
            name: sre-stuck-ebs-vols
          spec:
            affinity:
              nodeAffinity:
                preferredDuringSchedulingIgnoredDuringExecution:
                - preference:
                    matchExpressions:
                    - key: node-role.kubernetes.io/infra
                      operator: Exists
                  weight: 1
            containers:
            - command:
              - /bin/sh
              - /monitor/start.sh
              env:
              - name: AWS_SHARED_CREDENTIALS_FILE
                value: /secrets/aws/credentials.ini
              - name: AWS_CONFIG_FILE
                value: /secrets/aws/config.ini
              - name: PYTHONPATH
                value: /openshift-python/packages:/support/packages
              image: quay.io/app-sre/managed-prometheus-exporter-base:latest
              imagePullPolicy: IfNotPresent
              livenessProbe:
                failureThreshold: 2
                httpGet:
                  path: /
                  port: 8080
                initialDelaySeconds: 420
                periodSeconds: 360
                timeoutSeconds: 240
              name: main
              ports:
              - containerPort: 8080
                protocol: TCP
              readinessProbe:
                httpGet:
                  path: /
                  port: 8080
                initialDelaySeconds: 3
                timeoutSeconds: 240
              volumeMounts:
              - mountPath: /monitor
                name: monitor-volume
                readOnly: true
              - mountPath: /config
                name: envfiles
                readOnly: true
              - mountPath: /secrets
                name: secrets
                readOnly: true
              workingDir: /monitor
            dnsPolicy: ClusterFirst
            initContainers:
            - command:
              - /usr/local/bin/init.py
              - -r
              - /secrets/aws/config.ini
              - -a
              - /rawsecrets/aws_access_key_id
              - -A
              - /rawsecrets/aws_secret_access_key
              - -o
              - /secrets/aws/credentials.ini
              - -c
              - /config/env/CLUSTERID
              image: quay.io/app-sre/managed-prometheus-exporter-initcontainer:latest
              name: setupcreds
              volumeMounts:
              - mountPath: /rawsecrets
                name: awsrawcreds
                readOnly: true
              - mountPath: /secrets
                name: secrets
              - mountPath: /config
                name: envfiles
            restartPolicy: Always
            serviceAccountName: sre-stuck-ebs-vols
            tolerations:
            - effect: NoSchedule
              key: node-role.kubernetes.io/infra
              operator: Exists
            volumes:
            - name: awsrawcreds
              secret:
                secretName: sre-stuck-ebs-vols-aws-credentials
            - emptyDir: {}
              name: secrets
            - emptyDir: {}
              name: envfiles
            - configMap:
                name: sre-stuck-ebs-vols-code
              name: monitor-volume
        triggers:
        - type: ConfigChange
    - apiVersion: v1
      kind: Service
      metadata:
        labels:
          name: sre-stuck-ebs-vols
        name: sre-stuck-ebs-vols
        namespace: openshift-monitoring
      spec:
        ports:
        - name: http-main
          port: 80
          protocol: TCP
          targetPort: 8080
        selector:
          name: sre-stuck-ebs-vols
        sessionAffinity: None
        type: ClusterIP
    - apiVersion: monitoring.coreos.com/v1
      kind: ServiceMonitor
      metadata:
        labels:
          k8s-app: sre-stuck-ebs-vols
          name: sre-stuck-ebs-vols
        name: sre-stuck-ebs-vols
        namespace: openshift-monitoring
      spec:
        endpoints:
        - honorLabels: true
          interval: 2m
          port: http-main
          scheme: http
          scrapeTimeout: 2m
          targetPort: 0
        jobLabel: sre-stuck-ebs-vols
        namespaceSelector: {}
        selector:
          matchLabels:
            name: sre-stuck-ebs-vols
parameters:
- name: IMAGE_TAG
  required: true
  value: latest
